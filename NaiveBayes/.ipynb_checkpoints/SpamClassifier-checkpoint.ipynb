{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0296753e-bd97-4054-acc3-357beed9c324",
   "metadata": {},
   "source": [
    "## Goal ##\n",
    "\n",
    "Spam filtering is one of the most common real-world applications of machine learning, and most modern email providers already have systems that automatically flag unwanted messages as junk.\n",
    "\n",
    "In this project, a simple Naïve Bayes classifier is implemented to detect spam in SMS messages. The model is trained on a labeled dataset so that distinctions between spam and non-spam messages can be learned. Spam texts are often characterized by the presence of attention-grabbing words such as “FREE”, “WIN”, “PRIZE”, “CASH”, or by stylistic patterns such as extensive use of all caps and multiple exclamation marks.\n",
    "\n",
    "From a machine learning perspective, this is a binary classification task — each message is either spam or not spam. It also falls under supervised learning, since the model relies on a dataset of pre-labeled messages to learn patterns and make predictions on unseen data.\n",
    "\n",
    "# Overview\n",
    "\n",
    "This project has been broken down in to the following steps: \n",
    "\n",
    "- Step 0: Introduction to the Naive Bayes Theorem\n",
    "- Step 1.1: Understanding the dataset\n",
    "- Step 1.2: Data Preprocessing\n",
    "- Step 2.1: Bag of Words (BoW)\n",
    "- Step 2.2: Implementing BoW from scratch\n",
    "- Step 2.3: Implementing Bag of Words in scikit-learn\n",
    "- Step 3.1: Training and testing sets\n",
    "- Step 3.2: Applying Bag of Words processing to the dataset\n",
    "- Step 4.1: Bayes Theorem implementation from scratch\n",
    "- Step 4.2: Naive Bayes implementation from scratch\n",
    "- Step 5: Naive Bayes implementation using scikit-learn\n",
    "- Step 6: Evaluating the model\n",
    "- Step 7: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed48b16-0f63-470a-a458-88b8cd6526b0",
   "metadata": {},
   "source": [
    "### Step 0: Introduction to the Naive Bayes Theorem ###\n",
    "\n",
    "Bayes’ Theorem is regarded as one of the earliest algorithms for probabilistic inference. It was originally formulated by Reverend Thomas Bayes and, despite its age, continues to be highly effective in many applications.\n",
    "\n",
    "The idea can be understood more clearly through an example. Imagine a situation where security personnel are assigned to protect a political candidate during a public campaign event. Since the event is open to everyone, constant vigilance is required and each individual must be assessed for potential risk. Characteristics such as age, possession of a bag, or visible nervousness can be evaluated to determine whether a person poses a threat. If a person displays enough of these characteristics to cross a certain threshold of suspicion, precautionary action can be taken. This is essentially how Bayes’ Theorem operates: the probability of an event (a person being a threat) is calculated using the probabilities of related events (age, bag possession, nervous behavior, etc.).\n",
    "\n",
    "An important consideration is the assumption of independence among these features. For instance, nervousness in a child is far less concerning than nervousness in an adult. If nervousness alone were used as a predictor, many false alarms would be raised, as minors are more likely to appear nervous. By combining features such as age and nervousness, a more accurate judgment is obtained. This leads to the “naïve” assumption in Naïve Bayes: all features are treated as independent, even though in reality correlations often exist. While this assumption is an oversimplification, it allows the algorithm to remain efficient and surprisingly effective.\n",
    "\n",
    "Another intuitive example can be found in medical diagnosis. Suppose a patient is tested for a rare disease. The test is not perfect: it sometimes returns positive even when the patient is healthy (false positive), and sometimes returns negative when the patient is actually sick (false negative). Bayes’ Theorem allows the doctor to update the probability that the patient truly has the disease by combining:\n",
    "\n",
    "- The prior probability of having the disease (based on how common it is in the population), and the likelihood of the observed test result given the disease status. This way, rather than relying only on the test outcome, a more accurate probability can be calculated.\n",
    "\n",
    "In summary, Bayes’ Theorem provides a way to compute the probability of an event (e.g., a message being spam, a person being a threat, or a patient having a disease) from the joint probabilities of related evidence (e.g., keywords in text, observable behavior, or medical test results). The detailed mechanics of Bayes’ Theorem will be explored later, but first, attention will be given to understanding the dataset being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9dd46c-6397-48e8-98f1-0825fa1e74ea",
   "metadata": {},
   "source": [
    "### Step 1.1: Understanding the Dataset ###\n",
    "For this project, the SMS Spam Collection dataset from the UCI Machine Learning Repository is used. The UCI repository is widely known for hosting benchmark datasets frequently applied in experimental research. Abstract of the paper [abstract](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) and the original [compressed data file](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/) on the UCI site.\n",
    "\n",
    "Preview of the data:\n",
    "\n",
    "<img src=\"./images/dqnb.png\" height=\"1242\" width=\"1242\">\n",
    "\n",
    "The dataset contains two columns:\n",
    "\n",
    "The first column is the label:\n",
    "\n",
    "- 'ham' → the SMS is not spam\n",
    "\n",
    "- 'spam' → the SMS is spam\n",
    "\n",
    "The second column contains the actual text of the SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b15dd52-fd69-415d-a45b-cf463ee5b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the files in the current directory\n",
      "\n",
      " Volume in drive C is OS\n",
      " Volume Serial Number is B8A2-7C6A\n",
      "\n",
      " Directory of C:\\Users\\Asus\\NN_Projects\\NaiveBayes\n",
      "\n",
      "19.08.2025  11:16    <DIR>          .\n",
      "19.08.2025  11:25    <DIR>          ..\n",
      "19.08.2025  11:16    <DIR>          .ipynb_checkpoints\n",
      "19.08.2025  11:16    <DIR>          images\n",
      "19.08.2025  11:16    <DIR>          smsspamcollection\n",
      "19.08.2025  11:16            90.990 SpamClassifier.ipynb\n",
      "               1 File(s)         90.990 bytes\n",
      "               5 Dir(s)  432.080.314.368 bytes free\n",
      "\n",
      " List all the files inside the smsspamcollection directory\n",
      "\n",
      " Volume in drive C is OS\n",
      " Volume Serial Number is B8A2-7C6A\n",
      "\n",
      " Directory of C:\\Users\\Asus\\NN_Projects\\NaiveBayes\\smsspamcollection\n",
      "\n",
      "19.08.2025  11:16    <DIR>          .\n",
      "19.08.2025  11:16    <DIR>          ..\n",
      "19.08.2025  11:16           483.481 SMSSpamCollection\n",
      "               1 File(s)        483.481 bytes\n",
      "               2 Dir(s)  432.080.314.368 bytes free\n"
     ]
    }
   ],
   "source": [
    "# '!' allows us to run bash commands from jupyter notebook.\n",
    "print(\"List all the files in the current directory\\n\")\n",
    "!dir\n",
    "# The required data table can be found under smsspamcollection/SMSSpamCollection\n",
    "print(\"\\n List all the files inside the smsspamcollection directory\\n\")\n",
    "!dir smsspamcollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a044872-eebf-462b-81f7-958fc973427a",
   "metadata": {},
   "source": [
    ">**Instructions:**\n",
    "\n",
    "- Load the dataset into a pandas DataFrame using the read_table() function. \n",
    "- Since the data is tab-separated, pass \\t as the value for the sep parameter.\n",
    "- Provide custom column names by setting the names argument to ['label', 'sms_message'].\n",
    "- Finally, display the first five rows of the DataFrame to verify that the data has been imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422a9adf-2d40-46e7-ba83-828d96a30280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                        sms_message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Path to the dataset\n",
    "file_path = \"smsspamcollection/SMSSpamCollection\"\n",
    "\n",
    "df = pd.read_table(file_path, sep = '\\t', header = None, names = ['label', 'sms_message'])\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fcf8a-09f4-4fd3-af2d-2034c1e7df8d",
   "metadata": {},
   "source": [
    "### Step 1.2: Data Preprocessing ###\n",
    "\n",
    "Once the dataset has been explored, the label column is converted into binary values:  \n",
    "- **0** → _ham_ (not spam)  \n",
    "- **1** → _spam_  \n",
    "\n",
    "This conversion is performed to make the data directly usable for computation.  \n",
    "\n",
    "The reason for this step lies in how **scikit-learn and most machine learning libraries operate under the hood**. Internally, only **numerical representations** are handled. If string labels such as `\"ham\"` and `\"spam\"` were left unchanged, scikit-learn would automatically encode them, often casting them into float values or internally mapping them to integers without explicit visibility.  \n",
    "\n",
    "While the model would still be able to train and predict, leaving this process to the library can introduce hidden conversions that may cause problems later. For example, during the calculation of **performance metrics** (precision, recall, F1-score), mismatched or opaque encodings may lead to confusing results.  \n",
    "\n",
    "Converting categorical labels into integers beforehand is considered **best practice in ML pipelines**. Not only does it make the preprocessing explicit and transparent, it also aligns with how modern ML models and data pipelines are typically structured:  \n",
    "\n",
    "- **Transparency** – it is always clear what numeric value corresponds to each category.  \n",
    "- **Consistency** – the same encoding can be reused when new data arrives.  \n",
    "- **Compatibility** – some algorithms (e.g., logistic regression, Naïve Bayes) expect purely numeric input matrices.  \n",
    "- **Performance** – numerical operations are much faster than string operations.  \n",
    "\n",
    "In short, preprocessing steps like label encoding are not merely for convenience but are part of the broader practice of ensuring that all inputs to a model are **clean, consistent, and numerical**—which is essential for reliable model training and evaluation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc3b709-1d41-431e-9daf-0ed9c2c10a04",
   "metadata": {},
   "source": [
    ">**Instructions:**\n",
    "* Convert the values in the 'label' colum to numerical values using map method as follows:\n",
    "    - `{'ham':0, 'spam':1}` This maps the 'ham' value to 0 and the 'spam' value to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2462417-a9cb-435e-9f92-e259a9aa94c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87519c95-af80-4bba-9031-c47d267f76de",
   "metadata": {},
   "source": [
    "### Step 2.1: Bag of Words (BoW)  \n",
    "\n",
    "A dataset containing 5,572 SMS messages is being examined.  \n",
    "Since most machine learning algorithms are designed to operate on **numerical feature vectors**, a direct use of raw text is not possible.  \n",
    "Therefore, a transformation into a numerical representation is required before the data can be processed.  \n",
    "\n",
    "---\n",
    "\n",
    "### What is Bag of Words (BoW)?  \n",
    "\n",
    "The **Bag of Words (BoW)** model is considered one of the most fundamental text representation techniques in Natural Language Processing (NLP).  \n",
    "\n",
    "In this approach:  \n",
    "- Each text document is regarded as a *“bag”* of words.  \n",
    "- The order of the words is disregarded.  \n",
    "- Only the **frequency** (or presence) of words is retained.  \n",
    "\n",
    "As a result, a **document-term matrix (DTM)** is produced, where:  \n",
    "- Each **row** is associated with a document (e.g., an SMS message).  \n",
    "- Each **column** corresponds to a unique word (token).  \n",
    "- Each **cell** contains the frequency of that word in the corresponding document.  \n",
    "\n",
    "---\n",
    "\n",
    "### Why is BoW important?  \n",
    "\n",
    "- **Numerical input for ML models**  \n",
    "  By this method, text is converted into numeric input that can be used by algorithms such as Naïve Bayes, Logistic Regression, and SVMs.  \n",
    "\n",
    "- **Foundation for other techniques**  \n",
    "  More advanced representations such as **TF-IDF**, **Word2Vec**, **GloVe**, and contextual embeddings (**BERT, GPT**) have been built on top of this basic idea.  \n",
    "\n",
    "- **Efficiency & simplicity**  \n",
    "  Despite its simplicity, good performance is often achieved in classification tasks (e.g., spam detection).  \n",
    "\n",
    "- **Interpretability**  \n",
    "  Since each feature corresponds directly to an actual word, interpretability is enhanced. For instance, if words like “win” or “prize” are assigned high weights in a classifier, the decision can be intuitively understood.  \n",
    "\n",
    "---\n",
    "\n",
    "### Example  \n",
    "\n",
    "Consider the following four short documents:  \n",
    "\n",
    "```python\n",
    "['Hello, how are you!',  \n",
    " 'Win money, win from home.',  \n",
    " 'Call me now',  \n",
    " 'Hello, call you tomorrow?']\n",
    " ```\n",
    "Vocabulary Creation\n",
    "```python\n",
    "From the text collection, the following unique tokens (after lowercasing and ignoring punctuation) are extracted:\n",
    " ['hello', 'how', 'are', 'you', 'win', 'money', 'from', 'home', 'call', 'me', 'now', 'tomorrow']\n",
    " ```\n",
    "\n",
    "Each document is then represented as a vector of token frequencies:  \n",
    "\n",
    "| Document                         | hello | how | are | you | win | money | from | home | call | me | now | tomorrow |\n",
    "|----------------------------------|:-----:|:---:|:---:|:---:|:---:|:-----:|:----:|:----:|:----:|:--:|:---:|:--------:|\n",
    "| Doc1: \"Hello, how are you!\"      |   1   |  1  |  1  |  1  |  0  |   0   |  0   |  0   |  0   | 0  |  0  |    0     |\n",
    "| Doc2: \"Win money, win from home.\"|   0   |  0  |  0  |  0  |  2  |   1   |  1   |  1   |  0   | 0  |  0  |    0     |\n",
    "| Doc3: \"Call me now\"              |   0   |  0  |  0  |  0  |  0  |   0   |  0   |  0   |  1   | 1  |  1  |    0     |\n",
    "| Doc4: \"Hello, call you tomorrow?\"|   1   |  0  |  0  |  1  |  0  |   0   |  0   |  0   |  1   | 0  |  0  |    1     |\n",
    "\n",
    "---\n",
    "### Limitations of BoW\n",
    "\n",
    "Although widely used, several limitations are associated with BoW:\n",
    "\n",
    "- **Loss of word order and context**\n",
    "Sentences such as “dog bites man” and “man bites dog” are represented identically.\n",
    "\n",
    "- **High dimensionality**\n",
    "A large vocabulary produces very sparse matrices, which can increase memory usage and computation time.\n",
    "\n",
    "- **No semantic understanding**\n",
    "Words such as “reward” and “prize” are treated as unrelated, even though their meanings are close.\n",
    "\n",
    "- **Sensitivity to vocabulary**\n",
    "Misspellings, capitalization, or rare words can result in unnecessary dimensions.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd009e-c62c-47a1-bf10-efd2c32ecefe",
   "metadata": {},
   "source": [
    "### Step 2.2: Implementing Bag of Words from Scratch  \n",
    "\n",
    "Before a library implementation, a minimal Bag of Words pipeline is constructed manually to reveal the operations performed behind the scenes.\n",
    "\n",
    "**Step 1 — Lowercasing**  \n",
    "All strings in the document set are converted to lower case. This normalization ensures that tokens differing only by case (e.g., “Hello” vs “hello”) are treated identically.\n",
    "\n",
    "**Given documents**\n",
    "```python\n",
    "documents = [\n",
    "    'Hello, how are you!',\n",
    "    'Win money, win from home.',\n",
    "    'Call me now.',\n",
    "    'Hello, Call hello you tomorrow?'\n",
    "]\n",
    "\n",
    "```\n",
    ">**Instructions:**\n",
    "* All strings are to be converted to lower case and stored in a list named `lower_case_documents`.\r",
    "* The Python string method **.lower()** is to be applied (e.g., via a list comprehension)..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831b9d12-9c06-4a63-9ef0-29c5cbce5aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
     ]
    }
   ],
   "source": [
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "\n",
    "lower_case_documents = [i.lower() for i in documents]\n",
    "print(lower_case_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e06350-aa73-4320-b1d3-cb3d8d2da008",
   "metadata": {},
   "source": [
    "**Step 2 - Removing all punctuation**\n",
    "- After converting to lowercase, punctuation symbols are removed to prevent them from being treated as distinct tokens. For example, `\"hello!\"` and `\"hello\"` should not be considered different words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b1fa1a-5c37-45c6-8554-971341945042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string \n",
    "\n",
    "# Remove all punctuation from the strings in the document set. \n",
    "# Save them into a list \n",
    "sans_punctuation_documents = []\n",
    "for i in lower_case_documents:\n",
    "    sans_punctuation_documents.append(re.sub(r'[^\\w\\s]', '', i))\n",
    "\n",
    "print(sans_punctuation_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd36995-8059-42fc-af6e-7f1e06823e81",
   "metadata": {},
   "source": [
    "**Step 3: Tokenization**\n",
    "\n",
    "Tokenization refers to the process of splitting sentences into individual words (tokens). A **delimeter** is used to determine where one word ends and the next begins. In most cases, a space (`\" \"`) is used as the delimeter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa75c9b-3a86-4ef0-8c81-7baa942937e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the strings stored in sans_punctuation_documents using the \n",
    "# split() method. Store the final document set in a list \n",
    "\n",
    "#preprocessed_documents = [i.split() for i in sans_punctuation_documents]\n",
    "\n",
    "preprocessed_documents = []\n",
    "for i in sans_punctuation_documents:\n",
    "    preprocessed_documents.append(i.split())\n",
    "\n",
    "print(preprocessed_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41cba46-6fd0-4ace-9f06-8f018baabe44",
   "metadata": {},
   "source": [
    "\n",
    "**Step 4: Counting Frequencies**  \r\n",
    "\r\n",
    "Once the document set has been tokenized into lists of words, the next step is to determine how often each word appears in every document. This is accomplished by computing the frequency of occurrence for each token.  \r\n",
    "\r\n",
    "For this purpose, the `Counter` class from Python’s `collections` library is utilized. The `Counter` automatically tallies the frequency of each element in a list and produces a dictionary-like object, where the keys represent the items being counted and the values correspond to their respective counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60655b1b-8303-4e9e-8759-194421c04a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n",
      " Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n",
      " Counter({'call': 1, 'me': 1, 'now': 1}),\n",
      " Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"
     ]
    }
   ],
   "source": [
    "# Using the Counter() method and preprocessed_documents as input\n",
    "# create a dictionary with the keys being each word in each document \n",
    "# and the corresponding values being the frequency of occurrence of that word. \n",
    "# save each Counter dictionary as an item in a list called frequency_list\n",
    "\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "frequency_list = [Counter(i) for i in preprocessed_documents]\n",
    "\n",
    "pprint.pprint(frequency_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3e179-f648-429a-9d92-686de2406005",
   "metadata": {},
   "source": [
    "### Step 2.3: Implementing Bag of Words in scikit-learn ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeadb6d8-4492-4608-838c-b45e9c680be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we will look to create a frequency matrix on a smaller document set to make sure we understand how the \n",
    "document-term matrix generation happens. We have created a sample document set 'documents'.\n",
    "'''\n",
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee6eeb6-53e8-4e40-a6c0-f3668d991913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433b096-9e9c-4034-80ef-76c703b9e4b8",
   "metadata": {},
   "source": [
    "**Data preprocessing with CountVectorizer()**\n",
    "\n",
    "In Step 2.2, a simplified version of the `CountVectorizer()` method was implemented manually, which required preprocessing steps such as converting all text to lowercase and removing punctuation. Within `CountVectorizer()`, several parameters are provided to automatically handle these operations:  \n",
    "\n",
    "* **`lowercase = True`**  \n",
    "\n",
    "    By default, the `lowercase` parameter is set to `True`, ensuring that all text is automatically converted into lowercase form before further processing.  \n",
    "\n",
    "* **`token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b`**  \n",
    "\n",
    "    The `token_pattern` parameter is assigned a default regular expression `(?u)\\\\b\\\\w\\\\w+\\\\b`. With this setting, punctuation marks are ignored and treated as delimiters, while alphanumeric strings of length two or greater are identified as valid tokens.  \n",
    "\n",
    "* **`stop_words`**  \n",
    "\n",
    "    The `stop_words` parameter, when specified as `'english'`, removes all tokens from the document set that match a predefined list of English stop words contained within scikit-learn. Since the dataset in question consists of short SMS messages rather than larger bodies of text such as emails, this parameter is not applied in this case.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3753c41-78fc-4b7f-8f6c-697ea7877cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Get parameters of the 'count_vector' object which is an instance of 'CountVectorizer()'\n",
    "'''\n",
    "count_vector.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d646255-455c-443d-bdac-31b7f4ec7e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['are', 'call', 'from', 'hello', 'home', 'how', 'me', 'money',\n",
       "       'now', 'tomorrow', 'win', 'you'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the document dataset to the CountVectorizer() object\n",
    "# Get the list of words which have been categorized as features by using the method get_feature_names_out()\n",
    "\n",
    "count_vector.fit(documents)\n",
    "count_vector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee042ae-07be-4028-a174-63af3bd5178b",
   "metadata": {},
   "source": [
    "The `get_feature_names_out()` method returns the output feature names for this dataset, which is the set of words that make up the vocabulary for 'documents'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ba085b-fea5-4fc0-a639-a20ed3a0a8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix with the rows being each of the 4 documents, and the columns being each word. \n",
    "# The corresponding (row, column) value is the frequency of occurrance of that word\n",
    "# Achieve this using the `transform()` method and passing in the document data set as the \n",
    "# argument. The `transform()` method returns a matrix of numpy integers\n",
    "# we can convert this to an array using `toarray()`. Call the array `doc_array`.\n",
    "\n",
    "doc_array = count_vector.transform(documents).toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56152410-5964-4bb5-99bb-8731fe64a6e0",
   "metadata": {},
   "source": [
    "Above a clean representation of the documents in terms of the frequency distribution of the words in them can be seen. To make it easier to understand the next step is to convert this array into a dataframe and name the columns appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acadc203-ec93-4226-aa88-03b2867fe166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conver the array obtained, loaded into doc_array into a dataframe\n",
    "# set the column names to the word names (which were computed earlier using get_feature_names_out())\n",
    "\n",
    "feature_names = count_vector.get_feature_names_out()\n",
    "frequency_matrix = pd.DataFrame(doc_array, columns = feature_names)\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69f0ef-72eb-4015-86e2-ab1690dd80a9",
   "metadata": {},
   "source": [
    "One limitation of applying this method directly is that in the case of very large text datasets (for instance, collections of news articles or large volumes of emails), certain words will naturally occur far more frequently simply due to the structure of the language. Words such as *is*, *the*, *an*, pronouns, and other grammatical constructs can dominate the matrix and distort the analysis.  \n",
    "\n",
    "To mitigate this issue, two approaches are commonly employed:  \n",
    "\n",
    "- The `stop_words` parameter can be set to `'english'`. When enabled, all words that appear in scikit-learn’s built-in list of English stop words are automatically removed from the input text.  \n",
    "\n",
    "- The `TfidfVectorizer` method can be applied as an alternative. This approach reduces the weight of very frequent words and increases the importance of less common but informative words. While powerful, the details of this method are beyond the current lesson’s scope.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdfbd3-f1d2-4452-b449-52de4e643a35",
   "metadata": {},
   "source": [
    "### Step 3.1: Training and testing sets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cca35a93-5512-4bfa-a905-7a64b1031338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'sms_message'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37a6c3d3-bfc7-48f2-ba8e-1c25a3f98470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 5572\n",
      "Number of rows in the training set: 3733\n",
      "Number of rows in the test set: 1839\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into a training and testing set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"sms_message\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "\n",
    "print(f'Number of rows in the total set: {df.shape[0]}')\n",
    "print(f'Number of rows in the training set: {X_train.shape[0]}')\n",
    "print(f'Number of rows in the test set: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb0bad-e394-40a4-ab4f-db669edf2c5f",
   "metadata": {},
   "source": [
    "### Step 3.2: Applying Bag of Words processing to our dataset. ###\n",
    "\n",
    "The data has been splitted, the next objective is to follow the steps from Step 2: Bag of words and convert the data into the desired matrix format. To do this we will be using `CountVectorizer()` as we did before. There are two  steps to consider here:\n",
    "\n",
    "* Firstly, we have to fit our training data (`X_train`) into `CountVectorizer()` and return the matrix.\n",
    "* Secondly, we have to transform our testing data (`X_test`) to return the matrix. \n",
    "\n",
    "Note that `X_train` is our training data for the 'sms_message' column in our dataset and we will be using this to train our model. \n",
    "\n",
    "`X_test` is our testing data for the 'sms_message' column and this is the data we will be using(after transformation to a matrix) to make predictions on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13847c-0188-4945-b40a-3242c0b218bb",
   "metadata": {},
   "source": [
    "First, we are learning a vocabulary dictionary for the training data \n",
    "and then transforming the data into a document-term matrix; secondly, for the testing data we are only \n",
    "transforming the data into a document-term matrix.\n",
    "\n",
    "This is similar to the process in Step 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59a6788-323c-4476-8fc5-2c6ad5e7ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f97754-d4ff-4d7b-9c5f-ecfadfff4550",
   "metadata": {},
   "source": [
    "## Step 4.1: Bayes Theorem Implementation from Scratch\n",
    "\n",
    "Once the dataset has been prepared in the required format, the next stage involves the implementation of the algorithm that will be applied to classify whether a message should be considered spam or not. At the beginning of this project, Bayes’ theorem was briefly introduced; however, more detailed attention will now be given.\n",
    "\n",
    "In simple terms, Bayes’ theorem is used to calculate the probability of an event occurring, conditioned on prior knowledge of related probabilities. The theorem is composed of two main components:\n",
    "\n",
    "- **Prior Probability (Priors):** Probabilities known beforehand or assumed from external knowledge.  \n",
    "- **Posterior Probability (Posterior):** Probabilities that are updated after observing evidence (computed using priors).\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Medical Test for Diabetes\n",
    "\n",
    "To demonstrate the concept, consider the following illustrative example: the probability of an individual having diabetes given that a medical test has returned a positive result. In medicine, such probabilistic reasoning is critical since it directly influences diagnostic decisions, often with life-or-death consequences.\n",
    "\n",
    "The following assumptions are made (note: purely hypothetical, not based on real data):\n",
    "\n",
    "- **P(D):** Probability of having diabetes.  \n",
    "  - Assumed to be 0.01 (1% of the general population).  \n",
    "\n",
    "- **P(Pos):** Probability of receiving a positive test result.  \n",
    "\n",
    "- **P(Neg):** Probability of receiving a negative test result.  \n",
    "\n",
    "- **P(Pos|D):** Probability of a positive result given that diabetes is present.  \n",
    "  - Value: 0.9 (i.e., the test is correct 90% of the time for diabetic individuals).  \n",
    "  - This is also known as **Sensitivity** or **True Positive Rate (TPR)**.  \n",
    "\n",
    "- **P(Neg|~D):** Probability of a negative result given that diabetes is absent.  \n",
    "  - Value: 0.9 (i.e., the test is correct 90% of the time for non-diabetic individuals).  \n",
    "  - This is also known as **Specificity** or **True Negative Rate (TNR)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "- If **specificity** is high but **sensitivity** is low, false negatives are more likely (patients with diabetes may be missed).  \n",
    "- If **sensitivity** is high but **specificity** is low, false positives are more likely (healthy individuals may be incorrectly diagnosed).  \n",
    "- In machine learning, this trade-off between sensitivity and specificity is often managed using a **confusion matrix**, **precision/recall metrics**, and **ROC curves**.  \n",
    "- Bayes’ theorem forms the foundation of the **Naïve Bayes classifier**, a popular algorithm in Natural Language Processing (NLP) for tasks such as spam detection, sentiment analysis, and document classification.  \n",
    "\n",
    "---\n",
    "\n",
    "In the next step, the Bayes theorem will be implemented from scratch using this example, allowing us to observe how priors and conditional probabilities are combined to compute posterior probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482da6f-e7b6-40fd-99a7-74157f41e708",
   "metadata": {},
   "source": [
    "### Bayes’ Theorem Formula\n",
    "\n",
    "The Bayes theorem can be expressed as follows:\n",
    "\n",
    "<img src=\"images/bayes_formula.png\" height=\"242\" width=\"242\">\n",
    "\n",
    "Where each term is defined as:\n",
    "\n",
    "- **P(A):** The *prior probability* of event **A** occurring independently.  \n",
    "  - In this example, `P(D)` represents the probability of an individual having diabetes.  \n",
    "  - This value is assumed or given to us beforehand.\n",
    "\n",
    "- **P(B):** The *prior probability* of event **B** occurring independently.  \n",
    "  - In this example, `P(Pos)` represents the probability of a positive test result.  \n",
    "  - This value is computed using the **law of total probability**:\n",
    "    \\[\n",
    "    P(Pos) = P(Pos|D) \\cdot P(D) + P(Pos|\\sim D) \\cdot P(\\sim D)\n",
    "    \\]\n",
    "    This ensures all possible scenarios are accounted for.\n",
    "\n",
    "- **P(A|B):** The *posterior probability* that event **A** occurs given that event **B** has occurred.  \n",
    "  - In this example, `P(D|Pos)` represents the probability of an individual having diabetes **given that** the test returned positive.  \n",
    "  - This is the **value of interest** that is to be calculated.\n",
    "\n",
    "- **P(B|A):** The *likelihood probability* of event **B** occurring, given that event **A** has occurred.  \n",
    "  - In this example, `P(Pos|D)` represents the probability of receiving a positive result **given that diabetes is present**.  \n",
    "  - This value is provided (0.9 in our case).\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Insights\n",
    "\n",
    "- The **denominator** `P(B)` plays a critical role as a *normalizing constant*.  \n",
    "  Without it, probabilities could exceed 1 or fail to sum to 1 across all possible outcomes.  \n",
    "\n",
    "- A common pitfall is the **base rate fallacy**, where the low prevalence of a condition (e.g., diabetes at 1%) is ignored.  \n",
    "  Even with a highly accurate test, the posterior probability of truly having the condition may remain surprisingly low.\n",
    "\n",
    "- In practical machine learning applications, this formula is extended across multiple features under the **Naïve Bayes assumption** (features are conditionally independent).  \n",
    "  Despite the simplifying assumption, Naïve Bayes often performs remarkably well in high-dimensional domains such as text classification.\n",
    "\n",
    "---\n",
    "\n",
    "In the subsequent implementation, these probabilities will be plugged into the formula to compute `P(D|Pos)` step by step, before extending the approach to spam classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d4ce2-c76f-4d3d-8cbf-a9cde17f1152",
   "metadata": {},
   "source": [
    "### Substituting Values into Bayes’ Theorem\n",
    "\n",
    "Using the diabetes-testing example, the posterior probability is obtained by substituting the assumed values into Bayes’ theorem:\n",
    "\n",
    "$$\n",
    "P(D \\mid Pos) = \\frac{P(D)\\, P(Pos \\mid D)}{P(Pos)}\n",
    "$$\n",
    "\n",
    "The denominator $P(Pos)$ (the marginal probability of a positive test) is computed via **sensitivity** and **specificity** using the law of total probability:\n",
    "\n",
    "$$\n",
    "P(Pos) = P(D)\\cdot \\underbrace{P(Pos \\mid D)}_{\\text{Sensitivity}}\n",
    "+ P(\\lnot D)\\cdot \\underbrace{(1-\\text{Specificity})}_{P(Pos \\mid \\lnot D)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Numerical Illustration (with the assumed values)\n",
    "- $P(D) = 0.01$ (1% prevalence), hence $P(\\lnot D) = 0.99$.  \n",
    "- Sensitivity $= P(Pos \\mid D) = 0.9$.  \n",
    "- Specificity $= P(Neg \\mid \\lnot D) = 0.9 \\;\\Rightarrow\\; (1-\\text{Specificity}) = 0.1$.  \n",
    "\n",
    "**Step 1 — Compute $P(Pos)$:**\n",
    "\n",
    "$$\n",
    "P(Pos) = 0.01 \\cdot 0.9 + 0.99 \\cdot 0.1\n",
    "= 0.009 + 0.099 = 0.108\n",
    "$$\n",
    "\n",
    "**Step 2 — Compute $P(D \\mid Pos)$:**\n",
    "\n",
    "$$\n",
    "P(D \\mid Pos) = \\frac{0.01 \\cdot 0.9}{0.108}\n",
    "= \\frac{0.009}{0.108}\n",
    "\\approx 0.0833 \\; (8.33\\%)\n",
    "$$\n",
    "\n",
    "> **Interpretation:** Despite a highly accurate test (90% sensitivity & specificity), the **posterior** probability of actually having diabetes after a positive result is only about **8.33%** because the **base rate** (prevalence) is low. This is a classic illustration of the **base-rate fallacy**.\n",
    "\n",
    "---\n",
    "\n",
    "### Tips, Tricks, and Useful Technical Notes\n",
    "\n",
    "- **Likelihood Ratios (LR):** Computations can be stabilized by using  \n",
    "\n",
    "$$\n",
    "LR^+ = \\frac{\\text{Sensitivity}}{1-\\text{Specificity}}, \\quad\n",
    "LR^- = \\frac{1-\\text{Sensitivity}}{\\text{Specificity}}\n",
    "$$\n",
    "\n",
    "and updating **odds** instead of probabilities:  \n",
    "\n",
    "$$\n",
    "Posterior \\; Odds = Prior \\; Odds \\times LR^+\n",
    "$$\n",
    "\n",
    "for a positive test.  \n",
    "(With the numbers above, $LR^+ = 0.9/0.1 = 9$; Prior odds $= 0.01/0.99$.)\n",
    "\n",
    "- **PPV & NPV:**  \n",
    "  - **Positive Predictive Value (PPV)** equals $P(D \\mid Pos)$ (computed above).  \n",
    "  - **Negative Predictive Value (NPV)** equals $P(\\lnot D \\mid Neg)$, which increases as prevalence decreases and specificity/sensitivity improve.  \n",
    "\n",
    "- **Calibration Awareness:** Test performance (sensitivity/specificity) is population-dependent. If actual deployment data differ from the study population, recalibration may be required.  \n",
    "\n",
    "- **Numerical Stability (Logs):** In high-dimensional ML (e.g., Naïve Bayes for text), products of many small probabilities should be handled in **log-space** to avoid underflow:  \n",
    "\n",
    "$$\n",
    "\\log P = \\sum \\log P_i\n",
    "$$\n",
    "\n",
    "- **Connection to Spam Filtering (Naïve Bayes):**\n",
    "  - **Class Priors:** Set $P(\\text{spam})$ and $P(\\text{ham})$ from corpus frequencies (class imbalance matters).  \n",
    "  - **Conditional Likelihoods:** Estimate $P(\\text{token} \\mid \\text{class})$ with **Laplace smoothing** (e.g., $\\alpha=1$) to avoid zero probabilities.  \n",
    "  - **Decision Rule:** Compute log-posterior for each class and choose the larger. Thresholds can be shifted to trade off **precision vs. recall**.  \n",
    "  - **Feature Independence Assumption:** Although “naïve,” it often performs strongly in text due to sparse, high-dimensional signals.  \n",
    "\n",
    "---\n",
    "\n",
    "**Quick sanity check for your notes**\n",
    "- Posterior: $P(D \\mid Pos) = \\dfrac{P(D)\\,P(Pos \\mid D)}{P(Pos)}$  \n",
    "- Marginal: $P(Pos) = P(D)\\cdot \\text{Sensitivity} + P(\\lnot D)\\cdot (1-\\text{Specificity})$  \n",
    "\n",
    "> In subsequent sections, the same mechanics can be mirrored to implement a Naïve Bayes classifier from scratch for spam detection, swapping medical events with **class labels** (spam/ham) and **evidence** with token occurrences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bbcb2-1a5f-4645-996d-734925ebc479",
   "metadata": {},
   "source": [
    "### Tips, Tricks, and Useful Technical Notes\n",
    "\n",
    "- **Likelihood Ratios (LR):** Computations can be stabilized by using:\n",
    "\n",
    "$$\n",
    "LR^+ = \\frac{\\text{Sensitivity}}{1-\\text{Specificity}}, \\quad\n",
    "LR^- = \\frac{1-\\text{Sensitivity}}{\\text{Specificity}}\n",
    "$$\n",
    "\n",
    "and updating **odds** instead of probabilities:\n",
    "\n",
    "$$\n",
    "Posterior \\; Odds = Prior \\; Odds \\times LR^+\n",
    "$$\n",
    "\n",
    "For a positive test.  \n",
    "(With the numbers above, $LR^+ = 0.9/0.1 = 9$; Prior odds $= 0.01/0.99$.)\n",
    "\n",
    "- **PPV & NPV:**  \n",
    "  - **Positive Predictive Value (PPV):** $P(D \\mid Pos)$ (computed above).  \n",
    "  - **Negative Predictive Value (NPV):** $P(\\lnot D \\mid Neg)$, which increases as prevalence decreases and test accuracy improves.\n",
    "\n",
    "- **Calibration Awareness:** Test performance (sensitivity/specificity) is population-dependent. If actual deployment data differ from the study population, recalibration may be required.  \n",
    "\n",
    "- **Numerical Stability (Logs):** In high-dimensional ML (e.g., Naïve Bayes for text), products of many small probabilities should be handled in **log-space** to avoid underflow:\n",
    "\n",
    "$$\n",
    "\\log P = \\sum \\log P_i\n",
    "$$\n",
    "\n",
    "- **Connection to Spam Filtering (Naïve Bayes):**\n",
    "  - **Class Priors:** $P(\\text{spam})$ and $P(\\text{ham})$ are estimated from corpus frequencies (class imbalance matters).  \n",
    "  - **Conditional Likelihoods:** $P(\\text{token} \\mid \\text{class})$ estimated with **Laplace smoothing** (e.g., $\\alpha=1$) to avoid zero probabilities.  \n",
    "  - **Decision Rule:** Compute log-posterior for each class and select the larger. Thresholds can be shifted to adjust precision vs. recall.  \n",
    "  - **Feature Independence Assumption:** Although “naïve,” it often performs strongly in text due to sparse, high-dimensional signals.  \n",
    "\n",
    "---\n",
    "\n",
    "**Quick sanity check for your notes:**\n",
    "\n",
    "- Posterior:  \n",
    "  $$\n",
    "  P(D \\mid Pos) = \\dfrac{P(D)\\,P(Pos \\mid D)}{P(Pos)}\n",
    "  $$\n",
    "\n",
    "- Marginal:  \n",
    "  $$\n",
    "  P(Pos) = P(D)\\cdot \\text{Sensitivity} + P(\\lnot D)\\cdot (1-\\text{Specificity})\n",
    "  $$\n",
    "\n",
    "> In subsequent sections, the same mechanics can be mirrored to implement a Naïve Bayes classifier from scratch for spam detection, swapping medical events with **class labels** (spam/ham) and **evidence** with token occurrences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41b43ef5-6a6e-4cf4-9f3d-15d6feab2b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of getting a positive test result P(Pos) is: 0.108\n"
     ]
    }
   ],
   "source": [
    "# Calculate probability of getting a positive test result, P(Pos)\n",
    "\n",
    "# P(D)\n",
    "p_diabetes = 0.01\n",
    "\n",
    "# P(~D)\n",
    "p_no_diabetes = 0.99\n",
    "\n",
    "# Sensitivity or P(Pos|D)\n",
    "p_pos_diabetes = 0.9\n",
    "\n",
    "# Specificity or P(Neg/~D)\n",
    "p_neg_no_diabetes = 0.9\n",
    "\n",
    "# P(Pos)\n",
    "p_pos = (p_diabetes * p_pos_diabetes) + (p_no_diabetes * (1 - p_neg_no_diabetes))\n",
    "print(f'The probability of getting a positive test result P(Pos) is: {p_pos:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e9e99-e22e-496b-a403-bf534433b138",
   "metadata": {},
   "source": [
    "### Calculating Posterior Probabilities\n",
    "\n",
    "Using the previously defined values, the posterior probabilities can now be calculated:\n",
    "\n",
    "- **Probability of having diabetes, given a positive test result:**\n",
    "\n",
    "$$\n",
    "P(D \\mid Pos) = \\frac{P(D) \\cdot \\text{Sensitivity}}{P(Pos)}\n",
    "$$\n",
    "\n",
    "- **Probability of not having diabetes, given a positive test result:**\n",
    "\n",
    "$$\n",
    "P(\\lnot D \\mid Pos) = \\frac{P(\\lnot D) \\cdot (1 - \\text{Specificity})}{P(Pos)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Important Observation\n",
    "\n",
    "The sum of the posteriors will always equal **1**:\n",
    "\n",
    "$$\n",
    "P(D \\mid Pos) + P(\\lnot D \\mid Pos) = 1\n",
    "$$\n",
    "\n",
    "This property ensures that the probabilities are properly normalized after applying Bayes’ theorem.\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "- In binary classification, the two posterior probabilities ($P(D \\mid Pos)$ and $P(\\lnot D \\mid Pos)$) fully describe the model’s belief after observing the evidence.\n",
    "- In **multi-class Naïve Bayes**, this idea generalizes:  \n",
    "  $$\n",
    "  \\sum_{i=1}^k P(C_i \\mid x) = 1\n",
    "  $$\n",
    "  where $C_i$ are the possible classes and $x$ is the observed evidence.\n",
    "- Normalization is essential when comparing likelihoods across multiple classes; otherwise, raw values could exceed 1 or fail to sum correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f9e3a7a-9545-4a1d-9e3b-f3630bbd74c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of an individual having diabetes, given that that individual got a positive test result is: 0.083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compute the probability of an individual having diabetes, given that, that individual got a positive test result.\n",
    "# In other words, compute P(D|Pos).\n",
    "\n",
    "# The formula is: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\n",
    "\n",
    "# P(D|Pos)\n",
    "p_diabetes_pos = (p_diabetes * p_pos_diabetes) / p_pos\n",
    "print(f'Probability of an individual having diabetes, given '\n",
    "      f'that that individual got a positive test result is: {p_diabetes_pos:.3f}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e060e3b-3fe2-44d4-a788-d4a8e1814602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of an individual not having diabetes, given that individual got a positive test result is: 0.917\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Compute the probability of an individual not having diabetes, given that, that individual got a positive test result.\n",
    "In other words, compute P(~D|Pos).\n",
    "\n",
    "The formula is: P(~D|Pos) = (P(~D) * P(Pos|~D) / P(Pos)\n",
    "\n",
    "Note that P(Pos/~D) can be computed as 1 - P(Neg/~D). \n",
    "\n",
    "Therefore:\n",
    "P(Pos/~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1\n",
    "'''\n",
    "# P(Pos/~D)\n",
    "p_pos_no_diabetes = 0.1\n",
    "\n",
    "# P(~D|Pos)\n",
    "p_no_diabetes_pos = (p_no_diabetes * p_pos_no_diabetes) / p_pos\n",
    "print(f'Probability of an individual not having diabetes, given '\n",
    "      f'that individual got a positive test result is: {p_no_diabetes_pos:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad48ded-9cc3-4faf-8011-ab82207b1011",
   "metadata": {},
   "source": [
    "### What Does the Term *Naïve* in \"Naïve Bayes\" Mean?\n",
    "\n",
    "The term *naïve* refers to the simplifying assumption made by the algorithm: **all features are considered conditionally independent of each other, given the class label.**  \n",
    "\n",
    "This assumption is often unrealistic in practice, since real-world features are frequently correlated.  \n",
    "- In the diabetes example, only one feature (the test result) was considered, so independence was not an issue.  \n",
    "- Suppose an additional feature is introduced, such as *exercise habits*. Let this variable take a binary value:  \n",
    "  - `0` → the individual exercises less than or equal to 2 days a week  \n",
    "  - `1` → the individual exercises 3 or more days a week  \n",
    "  If both *test result* and *exercise* were to be included, Bayes’ theorem in its pure form would require modeling their **joint distribution**, which quickly becomes computationally expensive.\n",
    "\n",
    "Naïve Bayes extends Bayes’ theorem by assuming **feature independence**, so that the joint distribution can be factorized into simpler conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\dots, x_n \\mid C) = \\prod_{i=1}^{n} P(x_i \\mid C)\n",
    "$$\n",
    "\n",
    "This drastically reduces the number of parameters that need to be estimated.\n",
    "\n",
    "---\n",
    "\n",
    "### Why the \"Naïve\" Assumption Works Surprisingly Well\n",
    "\n",
    "- Even though strict independence is rarely true, the algorithm often performs remarkably well in practice—especially in **text classification** tasks (spam detection, sentiment analysis, document categorization).  \n",
    "- The assumption simplifies learning and inference, while still capturing enough signal to be useful in high-dimensional, sparse domains.  \n",
    "- With proper smoothing (e.g., **Laplace or Lidstone smoothing**), the algorithm can handle unseen words or rare features effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "- *Naïve* = assumes **feature independence**.  \n",
    "- Advantage: simplifies computations and prevents the curse of dimensionality.  \n",
    "- Limitation: performance may degrade if strong feature dependencies exist (e.g., overlapping signals).  \n",
    "- Despite being naïve, the method remains a **baseline model of choice** for many classification problems due to its speed, simplicity, and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed482c6-8cd9-470f-b76c-f9187c86355b",
   "metadata": {},
   "source": [
    "### Step 4.2: Naive Bayes implementation from scratch ###\n",
    "\n",
    "#### Extending Bayes’ Theorem to Multiple Features\n",
    "\n",
    "Once the foundations of Bayes’ theorem have been understood, the next step is to extend the framework to situations where **more than one feature** must be considered simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: Political Candidates and Word Usage\n",
    "\n",
    "Suppose two candidates are being observed:\n",
    "\n",
    "- Jill Stein (Green Party)  \n",
    "- Gary Johnson (Libertarian Party)  \n",
    "\n",
    "The following probabilities are assumed regarding the likelihood of each candidate using specific words during a speech:\n",
    "\n",
    "- Jill Stein:  \n",
    "  - $P(F \\mid J) = 0.1$ (says \"freedom\")  \n",
    "  - $P(I \\mid J) = 0.1$ (says \"immigration\")  \n",
    "  - $P(E \\mid J) = 0.8$ (says \"environment\")  \n",
    "\n",
    "- Gary Johnson:  \n",
    "  - $P(F \\mid G) = 0.7$  \n",
    "  - $P(I \\mid G) = 0.2$  \n",
    "  - $P(E \\mid G) = 0.1$  \n",
    "\n",
    "Additionally, it is assumed that the prior probability of either candidate giving a speech is equal:  \n",
    "\n",
    "$$\n",
    "P(J) = 0.5, \\quad P(G) = 0.5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Naïve Bayes?\n",
    "\n",
    "If the probability of Jill Stein using both the words *freedom* and *immigration* is to be computed, the **joint probability** must be considered.  \n",
    "This is where **Naïve Bayes** becomes valuable: it assumes that the features (*words*, in this case) are **conditionally independent** given the class (the candidate).\n",
    "\n",
    "The joint probability is then factorized as:\n",
    "\n",
    "$$\n",
    "P(F, I \\mid J) = P(F \\mid J) \\cdot P(I \\mid J)\n",
    "$$\n",
    "\n",
    "rather than modeling $P(F, I \\mid J)$ directly, which would otherwise require significantly more data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Naïve Bayes Formula\n",
    "\n",
    "The general form of the Naïve Bayes theorem is:\n",
    "\n",
    "<img src=\"images/naivebayes.png\" height=\"342\" width=\"342\">\n",
    "\n",
    "Where:  \n",
    "\n",
    "- $y$ → the **class variable** (in this example, the candidate).  \n",
    "- $x_1, x_2, \\dots, x_n$ → the **features** (in this example, the words spoken).  \n",
    "- Independence is assumed among the features, simplifying the computation of the posterior:\n",
    "\n",
    "$$\n",
    "P(y \\mid x_1, x_2, \\dots, x_n) \\propto P(y) \\prod_{i=1}^n P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- Without the independence assumption, modeling joint probabilities like $P(F, I, E \\mid J)$ would require **exponentially more data**.  \n",
    "- The *naïve* assumption dramatically reduces complexity while still achieving strong performance in many domains (especially text classification).  \n",
    "- In practice, features are rarely truly independent; however, Naïve Bayes remains robust, especially in **high-dimensional, sparse settings** such as Natural Language Processing.  \n",
    "- The decision rule is to select the class $y$ that maximizes the posterior probability:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{y}{\\operatorname{argmax}} \\; P(y) \\prod_{i=1}^n P(x_i \\mid y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00983cd-fdd4-4328-a4b5-090e7ff1a8f4",
   "metadata": {},
   "source": [
    "### Computing Posterior Probabilities with Naïve Bayes\n",
    "\n",
    "To classify which candidate is more likely to have said the words *freedom* and *immigration*, the posterior probabilities for each candidate must be computed.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Posterior for Jill Stein\n",
    "\n",
    "The probability that Jill Stein said both words (*freedom* and *immigration*) is:\n",
    "\n",
    "$$\n",
    "P(J \\mid F, I) = \\frac{P(J) \\cdot P(F \\mid J) \\cdot P(I \\mid J)}{P(F, I)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(J) = 0.5$  \n",
    "- $P(F \\mid J) = 0.1$  \n",
    "- $P(I \\mid J) = 0.1$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "P(J \\mid F, I) = \\frac{0.5 \\cdot 0.1 \\cdot 0.1}{P(F,I)}\n",
    "= \\frac{0.005}{P(F,I)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Posterior for Gary Johnson\n",
    "\n",
    "The probability that Gary Johnson said both words is:\n",
    "\n",
    "$$\n",
    "P(G \\mid F, I) = \\frac{P(G) \\cdot P(F \\mid G) \\cdot P(I \\mid G)}{P(F, I)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(G) = 0.5$  \n",
    "- $P(F \\mid G) = 0.7$  \n",
    "- $P(I \\mid G) = 0.2$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "P(G \\mid F, I) = \\frac{0.5 \\cdot 0.7 \\cdot 0.2}{P(F,I)}\n",
    "= \\frac{0.07}{P(F,I)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Denominator $P(F,I)$\n",
    "\n",
    "The denominator is the marginal probability of the words *freedom* and *immigration* being spoken in any speech, regardless of candidate:\n",
    "\n",
    "$$\n",
    "P(F,I) = P(J) \\cdot P(F \\mid J) \\cdot P(I \\mid J) \\;+\\; P(G) \\cdot P(F \\mid G) \\cdot P(I \\mid G)\n",
    "$$\n",
    "\n",
    "Substituting values:\n",
    "\n",
    "$$\n",
    "P(F,I) = (0.5 \\cdot 0.1 \\cdot 0.1) + (0.5 \\cdot 0.7 \\cdot 0.2)\n",
    "= 0.005 + 0.07 = 0.075\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Final Posterior Probabilities\n",
    "\n",
    "- For Jill Stein:\n",
    "\n",
    "$$\n",
    "P(J \\mid F, I) = \\frac{0.005}{0.075} \\approx 0.0667 \\; (6.67\\%)\n",
    "$$\n",
    "\n",
    "- For Gary Johnson:\n",
    "\n",
    "$$\n",
    "P(G \\mid F, I) = \\frac{0.07}{0.075} \\approx 0.9333 \\; (93.33\\%)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- If both words (*freedom* and *immigration*) are heard in a speech, **Gary Johnson** is far more likely (93.3%) to be the speaker compared to Jill Stein (6.7%).  \n",
    "- This illustrates how Naïve Bayes combines multiple features under the independence assumption to strongly favor one class over another, even when priors are equal.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a7adf42-5b53-431e-8116-ed7d57e57452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005000000000000001\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Compute the probability of the words 'freedom' and 'immigration' being said in a speech, or\n",
    "P(F,I).\n",
    "\n",
    "The first step is multiplying the probabilities of Jill Stein giving a speech with her individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_j_text\n",
    "\n",
    "The second step is multiplying the probabilities of Gary Johnson giving a speech with his individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_g_text\n",
    "\n",
    "The third step is to add both of these probabilities and you will get P(F,I).\n",
    "'''\n",
    "# P(J)\n",
    "p_j = 0.5\n",
    "\n",
    "# P(F/J)\n",
    "p_j_f = 0.1\n",
    "\n",
    "# P(I/J)\n",
    "p_j_i = 0.1\n",
    "\n",
    "p_j_text = p_j * p_j_f * p_j_i\n",
    "print(p_j_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078e6a36-5f99-434a-94c2-e57da2693ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "# P(G)\n",
    "p_g = 0.5\n",
    "\n",
    "# P(F/G)\n",
    "p_g_f = 0.7\n",
    "\n",
    "# P(I/G)\n",
    "p_g_i = 0.2\n",
    "\n",
    "p_g_text = p_g * p_g_f * p_g_i\n",
    "print(p_g_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b1bccfb-0695-413a-a09e-e0e5dff5686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words freedom and immigration being said are: 0.075\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 3: Compute P(F,I) and store in p_f_i\n",
    "'''\n",
    "p_f_i = p_j_text + p_g_text\n",
    "print(f'Probability of words freedom and immigration being said are: {p_f_i:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ca2c2-4227-467b-b46a-4406a3328a88",
   "metadata": {},
   "source": [
    "### Computing $P(J \\mid F,I)$ and $P(G \\mid F,I)$ (Naïve Bayes, two features)\n",
    "\n",
    "Using the Naïve Bayes assumption (features are conditionally independent given the class), the posteriors for **Jill Stein** ($J$) and **Gary Johnson** ($G$) conditioned on hearing the words *freedom* ($F$) and *immigration* ($I$) are obtained as follows:\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1 — Write the posterior forms\n",
    "$$\n",
    "P(J \\mid F,I) \\;=\\; \\frac{P(J)\\,P(F\\mid J)\\,P(I\\mid J)}{P(F,I)}, \n",
    "\\qquad\n",
    "P(G \\mid F,I) \\;=\\; \\frac{P(G)\\,P(F\\mid G)\\,P(I\\mid G)}{P(F,I)}.\n",
    "$$\n",
    "\n",
    "#### Step 2 — Compute the normalizer (marginal)\n",
    "By the law of total probability:\n",
    "$$\n",
    "P(F,I) \\;=\\; P(J)\\,P(F\\mid J)\\,P(I\\mid J) \\;+\\; P(G)\\,P(F\\mid G)\\,P(I\\mid G).\n",
    "$$\n",
    "\n",
    "With the given values\n",
    "\\[\n",
    "P(J)=P(G)=0.5,\\quad\n",
    "P(F\\mid J)=0.1,\\; P(I\\mid J)=0.1,\\quad\n",
    "P(F\\mid G)=0.7,\\; P(I\\mid G)=0.2,\n",
    "\\]\n",
    "it follows that\n",
    "$$\n",
    "P(F,I) \\;=\\; (0.5)(0.1)(0.1) \\;+\\; (0.5)(0.7)(0.2)\n",
    "\\;=\\; 0.005 \\;+\\; 0.070 \\;=\\; 0.075.\n",
    "$$\n",
    "\n",
    "#### Step 3 — Plug in and simplify\n",
    "- For Jill Stein:\n",
    "$$\n",
    "P(J \\mid F,I) \\;=\\; \\frac{(0.5)(0.1)(0.1)}{0.075}\n",
    "\\;=\\; \\frac{0.005}{0.075}\n",
    "\\;\\approx\\; 0.0667 \\;\\text{ (6.67\\%)}.\n",
    "$$\n",
    "\n",
    "- For Gary Johnson:\n",
    "$$\n",
    "P(G \\mid F,I) \\;=\\; \\frac{(0.5)(0.7)(0.2)}{0.075}\n",
    "\\;=\\; \\frac{0.070}{0.075}\n",
    "\\;\\approx\\; 0.9333 \\;\\text{ (93.33\\%)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- The posteriors sum to one:\n",
    "$$\n",
    "P(J \\mid F,I) + P(G \\mid F,I) = 0.0667 + 0.9333 \\approx 1.\n",
    "$$\n",
    "- The result strongly favors the candidate for whom both words are much more likely, despite equal class priors.  \n",
    "- This procedure scales to additional words by multiplying the corresponding likelihood terms under the independence assumption:\n",
    "$$\n",
    "P(y \\mid x_1,\\dots,x_n) \\propto P(y)\\prod_{i=1}^n P(x_i \\mid y).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d50a957-4f66-44cb-add5-c69d5971d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of Jill Stein saying the words Freedom and Immigration: 0.067\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Compute P(J|F,I) using the formula P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I) and store it in a variable p_j_fi\n",
    "'''\n",
    "p_j_fi = p_j_text / p_f_i\n",
    "print(f'The probability of Jill Stein saying the words Freedom and Immigration: {p_j_fi:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efbe0b49-8eac-46e0-8dfb-191d991639a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of Gary Johnson saying the words Freedom and Immigration: 0.933\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Compute P(G|F,I) using the formula P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I) and store it in a variable p_g_fi\n",
    "'''\n",
    "p_g_fi = p_g_text / p_f_i\n",
    "print(f'The probability of Gary Johnson saying the words Freedom and Immigration: {p_g_fi:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fa19f-34b3-41f6-a10c-240366df5921",
   "metadata": {},
   "source": [
    "### Conclusion: Posteriors Sum to 1 and Final Interpretation\n",
    "\n",
    "As observed—consistent with Bayes’ theorem—the **posterior probabilities are normalized** and their sum equals 1:\n",
    "\n",
    "$$\n",
    "P(J \\mid F, I) + P(G \\mid F, I) = 1.\n",
    "$$\n",
    "\n",
    "Based on the computed values:\n",
    "\n",
    "- \\(P(J \\mid F, I) \\approx 0.0667\\) (≈ **6.7%**)\n",
    "- \\(P(G \\mid F, I) \\approx 0.9333\\) (≈ **93.3%**)\n",
    "\n",
    "**Interpretation:** Under the given assumptions and the naïve (conditional independence) factorization, it is concluded that the probability that **Jill Stein** uses the words *“freedom”* and *“immigration”* in a speech is approximately **6.7%**, whereas the probability for **Gary Johnson** is approximately **93.3%**. These values sum to 1 by construction due to the normalization via the marginal \\(P(F, I)\\).\n",
    "\n",
    "---\n",
    "\n",
    "#### Quick Sanity Check (Optional)\n",
    "A compact verification is shown below to confirm normalization:\n",
    "$$\n",
    "\\frac{0.5 \\cdot 0.1 \\cdot 0.1}{0.075}\n",
    "\\;+\\;\n",
    "\\frac{0.5 \\cdot 0.7 \\cdot 0.2}{0.075}\n",
    "=\n",
    "\\frac{0.005 + 0.070}{0.075}\n",
    "=\n",
    "\\frac{0.075}{0.075}\n",
    "= 1.\n",
    "$$\n",
    "\n",
    "> **Note on rounding:** Percentages were rounded to one decimal place (6.7% and 93.3%). Minor differences (e.g., 6.6% vs. 6.7%) arise from rounding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44ca63-9ba9-4204-8286-5dc1dfd50b77",
   "metadata": {},
   "source": [
    "### A Generic Example of Naïve Bayes in Action\n",
    "\n",
    "Consider the scenario where the term *“Sacramento Kings”* is entered into a search engine.  \n",
    "For accurate results, the system must recognize the phrase as referring to the **NBA basketball team** rather than interpreting the words independently:\n",
    "\n",
    "- If *“Sacramento”* and *“Kings”* are treated separately, results could include:\n",
    "  - Images of the city of Sacramento (city landscapes).\n",
    "  - Images of crowns or historical monarchs (the word *“Kings”*).  \n",
    "- The desired outcome, however, is content related specifically to the **Sacramento Kings basketball team**.  \n",
    "\n",
    "This mismatch occurs because a *naïve* approach assumes independence between words, failing to capture their joint meaning as a phrase.\n",
    "\n",
    "---\n",
    "\n",
    "### Connection to Spam Detection\n",
    "\n",
    "The same principle applies when classifying text messages or emails as **spam** or **not spam**.  \n",
    "- The Naïve Bayes classifier evaluates each word **individually**, without modeling associations between words.  \n",
    "- While this independence assumption is simplistic, it is often effective in spam detection because certain *red-flag words* are strong indicators of spam.  \n",
    "\n",
    "**Examples of spam-trigger words:**\n",
    "- \"viagra\"\n",
    "- \"lottery\"\n",
    "- \"100% free\"\n",
    "- \"exclusive offer\"\n",
    "\n",
    "Even though context and word associations are ignored, the sheer presence of such words frequently provides enough evidence for accurate classification.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "- **Naïve Assumption:** Words/features are treated independently.  \n",
    "- **Strength:** Works well in domains where individual tokens carry strong predictive power (e.g., spam filters, sentiment analysis).  \n",
    "- **Limitation:** Fails to capture semantic relationships between words or phrases (e.g., “Sacramento Kings” as a unit).  \n",
    "\n",
    "Despite being “naïve,” this simplicity is what makes the algorithm computationally efficient and surprisingly powerful in many real-world text classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf4d87-50f9-46eb-95fb-67f7c5d2929d",
   "metadata": {},
   "source": [
    "## Step 5: Naïve Bayes Implementation with scikit-learn\n",
    "\n",
    "Returning to the spam-classification setting, the implementation can be carried out using **scikit-learn** without deriving the math from first principles. The `sklearn.naive_bayes` module provides several variants; for text data represented as **discrete counts**, the **Multinomial Naïve Bayes** classifier is typically employed. In contrast, **Gaussian Naïve Bayes** is suited to **continuous** features under a (conditional) normality assumption.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Multinomial NB for Text?\n",
    "- Text documents are commonly represented as **bag-of-words** or **n-gram** **count vectors** (non-negative integers).  \n",
    "- The Multinomial model assumes conditional independence of features and models counts with a multinomial likelihood, which aligns with token-count inputs (e.g., from `CountVectorizer` or `TfidfVectorizer` with non-negative features).\n",
    "\n",
    "> **Rule of thumb:**  \n",
    "> - Use **MultinomialNB** for **counts** or **TF–IDF** features (TF–IDF must be non-negative).  \n",
    "> - Use **GaussianNB** for continuous, real-valued features (e.g., sensor readings).  \n",
    "> - Use **BernoulliNB** for **binary** features (e.g., “word present/absent”).\n",
    "\n",
    "---\n",
    "\n",
    "### Minimal, Reproducible Pipeline (Recommended)\n",
    "\n",
    "It is considered best practice to wrap vectorization and classification inside a **Pipeline** so that preprocessing is tied to the model and cross-validation becomes straightforward.\n",
    "\n",
    "```python\n",
    "# (This is example code to paste into a code cell)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# X: list/Series of raw SMS texts\n",
    "# y: corresponding labels, e.g., 'spam' or 'ham'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Option A: Count-based features\n",
    "pipe_counts = Pipeline([(\"vect\", CountVectorizer(ngram_range=(1,1), lowercase=True)),\n",
    "(\"clf\", MultinomialNB(alpha=1.0))]) # Laplace smoothing by default \n",
    "\n",
    "# Option B: TF–IDF features (still valid with MultinomialNB as long as non-negative)\n",
    "pipe_tfidf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), lowercase=True)), (\"clf\", MultinomialNB(alpha=0.5))])\n",
    "\n",
    "# Fit and evaluate\n",
    "pipe = pipe_tfidf  # choose either pipe_counts or pipe_tfidf\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e179c545-cacb-4b92-b6c1-6ec4551f5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We have loaded the training data into the variable 'training_data' and the testing data into the \n",
    "variable 'testing_data'.\n",
    "\n",
    "Import the MultinomialNB classifier and fit the training data into the classifier using fit(). Name your classifier\n",
    "'naive_bayes'. You will be training the classifier using 'training_data' and y_train' from our split earlier. \n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "predictions = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0eb6b-3447-4244-b4c3-0bc4fc265ca6",
   "metadata": {},
   "source": [
    "## Step 6: Evaluating the Model\n",
    "\n",
    "With predictions obtained on the test set, the next objective is to **quantify performance** using appropriate metrics. Several complementary measures are typically reported, each highlighting a different aspect of classification quality—especially important under **class imbalance** (common in spam detection).\n",
    "\n",
    "---\n",
    "\n",
    "### Core Metrics\n",
    "\n",
    "**Accuracy**  \n",
    "The fraction of all predictions that are correct. Suitable when classes are balanced; misleading under heavy imbalance.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} \\;=\\; \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**Precision (Positive Predictive Value)**  \n",
    "Of the items predicted as *spam*, the proportion that truly are *spam*. High precision implies few **false positives**.\n",
    "\n",
    "$$\n",
    "\\text{Precision} \\;=\\; \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Recall (Sensitivity / True Positive Rate)**  \n",
    "Of the items that truly are *spam*, the proportion correctly identified as *spam*. High recall implies few **false negatives**.\n",
    "\n",
    "$$\n",
    "\\text{Recall} \\;=\\; \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**F1 Score**  \n",
    "The harmonic mean of precision and recall; balances both when a single number is needed.\n",
    "\n",
    "$$\n",
    "\\text{F1} \\;=\\; \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "> **Interpretation tip:**  \n",
    "> - **Precision** answers: “When the model says *spam*, how often is it correct?”  \n",
    "> - **Recall** answers: “Of all *spam*, how much did the model catch?”\n",
    "\n",
    "---\n",
    "\n",
    "### Why Accuracy Alone Is Insufficient (Imbalance)\n",
    "\n",
    "In skewed datasets (e.g., 2% spam, 98% ham), a classifier that labels everything as *ham* achieves 98% **accuracy** yet is useless. For such cases, **precision**, **recall**, and **F1** are preferred, often alongside **confusion matrices** to expose failure modes.\n",
    "\n",
    "A confusion matrix for the *spam* (positive) class:\n",
    "\n",
    "|                | Predicted: Spam | Predicted: Not Spam |\n",
    "|----------------|------------------|----------------------|\n",
    "| **Actual: Spam**     | TP               | FN                   |\n",
    "| **Actual: Not Spam** | FP               | TN                   |\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Evaluation Guidance (for Spam Detection)\n",
    "\n",
    "- **Report per-class metrics** and **macro/micro averages**  \n",
    "  - *Macro-F1*: unweighted average over classes (treats classes equally).  \n",
    "  - *Micro-F1*: aggregates contributions of all classes (favors majority class).\n",
    "\n",
    "- **Inspect Precision–Recall (PR) behavior**  \n",
    "  - **PR curves** and **Average Precision (AP)** are more informative than ROC under heavy imbalance.\n",
    "\n",
    "- **Threshold tuning**  \n",
    "  - Default predictions use argmax of posterior probabilities.  \n",
    "  - A custom threshold on \\( P(\\text{spam}\\mid x) \\) can be set to prioritize **recall** (catch more spam) or **precision** (reduce false alarms), depending on product needs.\n",
    "\n",
    "- **Calibration**  \n",
    "  - Naïve Bayes probabilities can be overconfident.  \n",
    "  - If decision thresholds or downstream costs rely on probability quality, apply **probability calibration** (e.g., `CalibratedClassifierCV` with Platt scaling or isotonic regression).\n",
    "\n",
    "- **Cross-validation and stratification**  \n",
    "  - Use **stratified** splits so class ratios are preserved across folds and the test set.  \n",
    "  - Report mean ± std over folds for stable estimates.\n",
    "\n",
    "- **Additional metrics (optional but useful)**  \n",
    "  - **Specificity (TNR)**: \\( \\frac{TN}{TN+FP} \\) — how well *ham* is protected from false spam flags.  \n",
    "  - **Matthews Correlation Coefficient (MCC)** — a balanced metric even with severe imbalance.  \n",
    "  - **Balanced Accuracy** — average of TPR and TNR.\n",
    "\n",
    "---\n",
    "\n",
    "### Minimal scikit-learn Evaluation Snippet\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f11834ac-e34f-41db-b0e0-2f86c78c237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.990\n",
      "Precision score: 0.975\n",
      "Recall score: 0.951\n",
      "F1 score: 0.963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(f'Accuracy score: {accuracy_score(y_test, predictions):.3f}')\n",
    "print(f'Precision score: {precision_score(y_test, predictions):.3f}')\n",
    "print(f'Recall score: {recall_score(y_test, predictions):.3f}')\n",
    "print(f'F1 score: {f1_score(y_test, predictions):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1767a-8799-4840-a449-78cda2458478",
   "metadata": {},
   "source": [
    "## Step 7: Conclusion\n",
    "\n",
    "The **Naïve Bayes algorithm** offers several advantages compared to other classification methods, particularly in text classification contexts such as spam detection:\n",
    "\n",
    "---\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- **Scalability with High-Dimensional Data**  \n",
    "  Each word in the vocabulary can be treated as a feature, resulting in thousands of dimensions.  \n",
    "  Naïve Bayes handles this efficiently without requiring complex feature selection.\n",
    "\n",
    "- **Robustness to Irrelevant Features**  \n",
    "  Even if many features carry little or no discriminative value, the algorithm is relatively unaffected, as informative features dominate the posterior computations.\n",
    "\n",
    "- **Simplicity and Ease of Use**  \n",
    "  Naïve Bayes typically performs well with minimal parameter tuning. Only in special cases (e.g., when the distribution of data is known to be Gaussian) is fine-tuning or choosing a different variant (Multinomial, Bernoulli, Gaussian) necessary.\n",
    "\n",
    "- **Low Risk of Overfitting**  \n",
    "  Due to its probabilistic and additive nature, Naïve Bayes tends to generalize well even with limited data compared to more flexible models.\n",
    "\n",
    "- **Efficiency**  \n",
    "  Training and inference are extremely fast, making the method well-suited for large-scale datasets or applications requiring real-time predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Note\n",
    "\n",
    "Naïve Bayes may appear simplistic—even “naïve”—due to its independence assumption. Yet, in practice, it remains **remarkably effective** for text classification and many other domains where features are numerous, sparse, and relatively independent.\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f78de6-690a-4c1b-bfbb-e884574d15ad",
   "metadata": {},
   "source": [
    "## Recap: Why Use Naïve Bayes?\n",
    "\n",
    "The **Naïve Bayes algorithm** applies probabilistic reasoning to classification tasks.  \n",
    "It belongs to the family of **supervised machine learning algorithms**, where training is performed on labeled data across binary or multi-class categories.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of Naïve Bayes\n",
    "\n",
    "- **Handles High-Dimensional Feature Spaces**  \n",
    "  Works effectively even when the number of features is extremely large, as in text classification where each unique word is a feature.\n",
    "\n",
    "- **Robust to Irrelevant Features**  \n",
    "  The presence of non-informative features has little impact because strong signals dominate the probability computation.\n",
    "\n",
    "- **Simplicity and Ease of Use**  \n",
    "  Performs well with minimal preprocessing and hyperparameter tuning. Different variants (Multinomial, Bernoulli, Gaussian) adapt to different data distributions.\n",
    "\n",
    "- **Low Risk of Overfitting**  \n",
    "  Compared to more flexible models, Naïve Bayes tends to generalize well and is less prone to fitting noise in the data.\n",
    "\n",
    "- **Efficiency**  \n",
    "  Training and inference are extremely fast, even on large datasets—making it suitable for real-time systems such as spam filters or document classifiers.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of Naïve Bayes\n",
    "\n",
    "- **Independence Assumption**  \n",
    "  The core assumption that features are independent given the class label rarely holds in practice.  \n",
    "  For example, the words *“New”* and *“York”* are strongly correlated; treating them independently can degrade performance.\n",
    "\n",
    "- **Zero-Frequency Problem**  \n",
    "  If a feature never appears in the training data for a class, its probability becomes zero, wiping out the entire posterior.  \n",
    "  → Solution: apply **Laplace/Lidstone smoothing** (e.g., `alpha` parameter in scikit-learn).\n",
    "\n",
    "- **Poor Probability Calibration**  \n",
    "  Although classification accuracy may be high, the predicted probabilities are often **overconfident**.  \n",
    "  → Solution: apply **probability calibration** methods (e.g., Platt scaling, isotonic regression).\n",
    "\n",
    "- **Limited Expressiveness**  \n",
    "  More advanced models (e.g., Logistic Regression, SVMs, Deep Neural Networks) can capture interactions between features, which Naïve Bayes ignores.\n",
    "\n",
    "- **Performance Drops with Correlated Features**  \n",
    "  If many features are highly dependent on each other, the independence assumption can distort posterior estimates and harm classification accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "Despite these limitations, Naïve Bayes remains a **reliable baseline algorithm** that is fast, interpretable, and robust in many high-dimensional, sparse domains (like spam detection).  \n",
    "It should often be the **first model tried** in text classification tasks, and its performance serves as a benchmark for evaluating more complex approaches.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
